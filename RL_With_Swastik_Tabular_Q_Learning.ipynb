{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_With_Swastik.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfMcbgBjaIjfYAzo8UCJrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swastiknath/Reinforcement_Learning_With_Swastik/blob/master/RL_With_Swastik_Tabular_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhNthc4Jd84V",
        "colab_type": "code",
        "outputId": "27015e0c-e364-467b-d7f0-f5364c370ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y ffmpeg xvfb python-opengl\n",
        "!apt-get update\n",
        "!apt-get install cmake\n",
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup\n",
        "!pip install gym[atari]\n",
        "import gym\n",
        "from gym import logger as logstat\n",
        "logstat.set_level(gym.logger.WARN)\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from gym.wrappers import Monitor\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.10)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 84 not upgraded.\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 84 not upgraded.\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.0.0)\n",
            "Requirement already satisfied: ez_setup in /usr/local/lib/python3.6/dist-packages (0.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.10)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (6.2.2)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEEMrrTTnDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9d84299d-adf7-4a8e-d80c-0fada8cd8dd4"
      },
      "source": [
        "def record_interval(n):\n",
        "    episode_interval = 100\n",
        "    return n % episode_interval == 0\n",
        "\n",
        "def wrap_env(env):\n",
        "\n",
        "  # env = Monitor(env, \"recording\")\n",
        "  env = Monitor(env, './video', video_callable=record_interval, force=True)\n",
        "  video_recorder = VideoRecorder(env, enabled=True)\n",
        "  return env, video_recorder\n",
        "display = Display(visible=0, size=(1400,900))\n",
        "display.start()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J13V89To0Zft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "class Player:\n",
        "  def __init__(self, env_name):\n",
        "    self.env = gym.make(env_name)\n",
        "    self.state = self.env.reset()\n",
        "    self.values = collections.defaultdict(float)\n",
        "    self.dbg_info_s = []\n",
        "\n",
        "  def init_env(self):\n",
        "    '''\n",
        "    Initializing the Player. \n",
        "    '''\n",
        "    action = self.env.action_space.sample()\n",
        "    old_state = self.state \n",
        "    new_state, reward, is_done, dbg_info = self.env.step(action)\n",
        "    self.state = self.env.reset() if is_done else new_state\n",
        "    self.dbg_info_s.append(dbg_info)\n",
        "    return (old_state, action, reward, new_state)\n",
        "\n",
        "  def max_action_value_and_action(self, state):\n",
        "    '''\n",
        "    Obtaining the best Action Values from the sampled Action Space and \n",
        "    obtaining the best action for stepping to. \n",
        "    '''\n",
        "    max_action_value, max_action = None, None\n",
        "    for act in range(self.env.action_space.n):\n",
        "      curr_act_val = self.values[(state, act)]\n",
        "      if max_action_value is None or curr_act_val > max_action_value:\n",
        "        max_action_value = curr_act_val\n",
        "        max_action = act  \n",
        "    return max_action_value, max_action  \n",
        "\n",
        "  def bellman_update(self, old_state, action, new_state, reward, GAMMA, learningRate):\n",
        "    ''' \n",
        "     Applying Bellman Update to the Action Value for each state.\n",
        "     Q(s, a) <- (1 - alpha) * Q(s, a) + (r + Gamma * max(Q(s', a')))\n",
        "     Parameter Space :\n",
        "     S = Current State \n",
        "     A = Action Space Sample for the Current State\n",
        "     S' = New State\n",
        "     R = Rewards Obtained\n",
        "     GAMMA = GAMMA VALUE for Bellman Updates (Required)\n",
        "     learningRate = ALPHA VALUE for smoother updates (Required)\n",
        "    '''\n",
        "    max_q_val,_ = self.max_action_value_and_action(new_state)\n",
        "    updt_q_val = reward + (GAMMA * max_q_val)\n",
        "    old_q_val = self.values[(old_state, action)]\n",
        "    self.values[(old_state, action)] = (1- learningRate) * old_q_val + updt_q_val * learningRate\n",
        "\n",
        "  def play_game_episodes(self, env):\n",
        "    '''\n",
        "    Play the agent in the envrionment and collect and step to the maximum \n",
        "    Value of Action and Action.\n",
        "    '''\n",
        "    total_rewards = 0.0\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      _, max_act = self.max_action_value_and_action(state)\n",
        "      new_state, reward, is_done, info = env.step(max_act)\n",
        "      total_rewards += reward\n",
        "      if is_done:\n",
        "        break\n",
        "      state = new_state\n",
        "    return total_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syJnAM_g2xPp",
        "colab_type": "code",
        "outputId": "bd26568f-87d9-42ec-d258-db8be1c524db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "if __name__ == \"__main__\":\n",
        "  ENV_NAME = \"FrozenLake-v0\"\n",
        "  test_env, video_record = wrap_env(gym.make(ENV_NAME))\n",
        "  GAMMA = 0.9\n",
        "  ALPHA = 0.2\n",
        "  TEST_EPS = 20\n",
        "  \n",
        "  progress = tqdm(total=TEST_EPS)\n",
        "  player = Player(ENV_NAME)\n",
        "  iter_num = 0\n",
        "  max_reward = 0.0\n",
        "  while True:\n",
        "    iter_num +=1\n",
        "    s, a, r, nxt_s = player.init_env() \n",
        "    progress.update(1)\n",
        "    player.bellman_update(s, a, nxt_s, r, GAMMA, ALPHA)\n",
        "    reward = 0.0\n",
        "    video_record.capture_frame()\n",
        "    for _ in range(TEST_EPS):\n",
        "      reward += player.play_game_episodes(test_env)\n",
        "      progress.set_description(\"Episode Reward Collected: %.2f\" % reward)\n",
        "    reward /= TEST_EPS\n",
        "    if reward > max_reward:\n",
        "      video_record.capture_frame()\n",
        "      old_reward = max_reward\n",
        "      max_reward = reward \n",
        "      print(\"Reward Updated from {} --> {}\".format(old_reward, reward))\n",
        "    if reward > 0.9:\n",
        "      video_record.capture_frame()\n",
        "      print(\"Environment Solved with reward : {}\".format(max_reward))\n",
        "      progress.close()\n",
        "      break;\n",
        "  test_env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode Reward Collected: 0.00: : 51it [00:02, 22.23it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}